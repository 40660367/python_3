{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词与TAGGING\n",
    "%config ZMQInteractiveShell.ast_node_interactivity='all'\n",
    "import spacy\n",
    "text = \"The sequel, Yes, Prime Minister, ran from 1986 to 1988. In total there were 38 episodes, of which all but one lasted half an hour. Almost all episodes ended with a variation of the title of the series spoken as the answer to a question posed by the same character, Jim Hacker. Several episodes were adapted for BBC Radio, and a stage play was produced in 2010, the latter leading to a new television series on UKTV Gold in 2013.\"\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,',', ent.label_)\n",
    "    \n",
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "\n",
    "for token in doc[:10]:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))\n",
    "    \n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    \n",
    "doc.sents\n",
    "list(doc.sents)\n",
    "\n",
    "newdoc = nlp(list(doc.sents)[0].text)\n",
    "\n",
    "for token in newdoc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n",
    "displacy.render(newdoc, style='dep', jupyter=True, options={'distance': 90})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeding可视化\n",
    "import numpy as np\n",
    "embedding = np.array([])\n",
    "word_list = []\n",
    "for token in doc:\n",
    "    if not(token.is_punct) and not(token.text in word_list):\n",
    "        word_list.append(token.text)\n",
    "        \n",
    "for word in word_list:\n",
    "    embedding = np.append(embedding, nlp.vocab[word].vector)\n",
    "embedding.shape\n",
    "embedding = embedding.reshape(len(word_list), -1)\n",
    "embedding.shape\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE()\n",
    "low_dim_embedding = tsne.fit_transform(embedding)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.pdf'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "    plt.savefig(filename)\n",
    "plot_with_labels(low_dim_embedding, word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#向量\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "print(nlp.vocab['minister'].vector)\n",
    "\n",
    "dog = nlp.vocab[\"dog\"]\n",
    "cat = nlp.vocab[\"cat\"]\n",
    "apple = nlp.vocab[\"apple\"]\n",
    "orange = nlp.vocab[\"orange\"]\n",
    "\n",
    "dog.similarity(cat)\n",
    "dog.similarity(apple)\n",
    "dog.similarity(orange)\n",
    "apple.similarity(orange)\n",
    "\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "1 - cosine(dog.vector, cat.vector)\n",
    "def vector_similarity(x, y):\n",
    "    return 1 - cosine(x, y)\n",
    "vector_similarity(dog.vector, apple.vector)\n",
    "\n",
    "def make_guess_word(words):\n",
    "    [first, second, third] = words\n",
    "    return nlp.vocab[first].vector - nlp.vocab[second].vector + nlp.vocab[third].vector\n",
    "\n",
    "def get_similar_word(words, scope=nlp.vocab):\n",
    "    \n",
    "    guess_word = make_guess_word(words)\n",
    "\n",
    "    similarities = []\n",
    "\n",
    "    for word in scope:\n",
    "        if not word.has_vector:\n",
    "            continue\n",
    "\n",
    "        similarity = vector_similarity(guess_word, word.vector)\n",
    "        similarities.append((word, similarity))\n",
    "\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda item: -item[1])\n",
    "    print([word[0].text for word in similarities[:10]])\n",
    "\n",
    "words = [\"king\", \"queen\", \"woman\"]\n",
    "get_similar_word(words)\n",
    "\n",
    "words = [\"Paris\", \"London\", \"England\"]\n",
    "get_similar_word(words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
